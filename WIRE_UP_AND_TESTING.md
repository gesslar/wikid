# ACTUAL_TESTING.md

## The Real Deal: How to Actually Test This Toolkit

*Unlike TESTING.txt (which is fucking gold and stays!), this is your practical guide for adding new stuff without breaking everything.*

---

## üéØ **When Adding New Methods/Classes**

### **Step 0: New Project / Fresh Scaffold Checklist**

Before anyone (human or otherwise) starts wiring in features, make sure the repo baseline is ready for the automation in the later steps:

1. **Copy the tooling files** into the project root:
   - `tsconfig.types.json` (the one that emits `.d.ts` + `.d.ts.map`)
   - `eslint.config.js` (or merge its relevant bits if you already have one)
   - `WIRE_UP_AND_TESTING.md` itself, so future contributors know the drill.
2. **Update `package.json` scripts** so there is a `types:build` entry:

   ```json
   {
     "scripts": {
       "types:build": "tsc -p tsconfig.types.json && eslint --fix \"src/types/**/*.d.ts\""
     }
   }
   ```

   Adjust the quoting if you‚Äôre on Windows, but keep the same two commands.
3. **Lock in the directory layout**:
   - Runtime code lives under `src/lib/**` + `src/index.js`.
   - Generated declarations land in `src/types/**` (commit them).
   - Consumers import from `src/index.js`, so always expose new classes there.
4. **Sanity check the pipeline**:

   ```bash
   npm install
   npm run types:build
   npm run lint
   npm test
   ```

   Running the types build once up front confirms everything is wired, and it gives you baseline `.d.ts` files with source maps for editors.

With that scaffold in place, the rest of this playbook (Steps 1‚Äì5) applies cleanly.

### **Step 1: Validate the Logic Thoroughly**

Before you write a single test, **audit your implementation**:

- ‚úÖ **Edge Cases**: What happens with `null`, `undefined`, empty arrays, zero values?
- ‚úÖ **Type Coercion**: Does your method handle non-expected types gracefully?
- ‚úÖ **Error Boundaries**: Where can this blow up? Handle it or document it.
- ‚úÖ **Performance**: Any obvious bottlenecks or inefficiencies?
- ‚úÖ **API Consistency**: Does it match the existing toolkit's patterns and voice?

**üö® CRITICAL**: Look for patterns like:

- Destructuring from `null` (use `[]` instead)
- Silent type failures (decide: throw or coerce?)
- Missing validation on user inputs
- Async operations without proper error handling

### **Step 2: Wire Up the Class to `index.js`**

Add your new class to the exports:

```javascript
// src/index.js
export { default as YourNewClass } from "./lib/YourNewClass.js"
```

### **Step 3: Keep the Type Definitions in Sync**

`src/types` is now generated. Do **not** hand-edit anything in there. Instead:

1. **Update and polish the JSDoc in your JS sources whenever you add or modify classes, methods, or exports.** This ensures that type definitions generated by `tsc --allowJs` are always accurate and up to date. The tighter the annotations, the better the emitted `.d.ts`.
2. **Regenerate everything** after touching `src/lib/**` or `src/index.js`:

  ```bash
  npm run types:build
  ```

  This runs `tsc -p tsconfig.types.json` (to emit `.d.ts` + `.d.ts.map`) and then `eslint --fix "src/types/**/*.d.ts"` so the generated files match our lint rules.
3. **Verify the output** in `src/types/lib/*.d.ts` plus `src/types/index.d.ts`. You should see a matching `.d.ts.map` that points back to your JS implementation.

#### **JSDoc Style Guidelines for JavaScript Files**

These rules keep the generator honest:

- ‚ùå Never use `any` or `*` ‚Üí use `unknown`.
- ‚ùå Never use `[]` ‚Üí use `Array<Type>`.
- ‚ùå Never use bare `Function`/`Object` ‚Üí describe the signature/shape explicitly.

```javascript
// ‚ùå BAD
/** @param {any} data @param {Function} callback */

// ‚úÖ GOOD
/** @param {unknown} data @param {(result: string) => void} callback */
```

Why? It forces better inference, keeps the emitted declarations stable, and avoids eslint yelling at you.

#### **Need richer types?**

Add more detailed JSDoc (overloads, generics via `@template`, etc.) directly in the JS source. Re-run `npm run types:build` and the declarations will pick it up automatically.

### **Step 4: Check the Bundle Exports**

Because `tsc` reads `src/index.js`, any new export automatically shows up in `src/types/index.d.ts` after you run `npm run types:build`. Double-check that:

- Your new class is exported from `src/index.js` (Step 2).
- The regenerated `src/types/index.d.ts` now re-exports it from `./lib/...`.
- The matching `.d.ts.map` exists so F12 lands in the JS source.

If something‚Äôs missing, it usually means the class wasn‚Äôt exported from `src/index.js` or the JSDoc left TypeScript guessing. Fix the source, rerun the build, done.

### **Step 5: Write Comprehensive Tests**

Create `tests/unit/YourNewClass.test.js`:

#### **Test Structure Template:**

```javascript
#!/usr/bin/env node

import { describe, it } from "node:test"
import assert from "node:assert/strict"

import { YourNewClass } from "../../src/index.js"

describe("YourNewClass", () => {
  describe("import", () => {
    it("can be imported from package", () => {
      assert.ok(YourNewClass)
      assert.equal(typeof YourNewClass, "function")
    })
  })

  describe("methodName()", () => {
    it("handles normal cases", () => {
      // Test the happy path
      const result = YourNewClass.methodName("input")
      assert.equal(result, "expected")
    })

    it("handles edge cases", () => {
      // Test the weird stuff
      assert.equal(YourNewClass.methodName(""), "")
      assert.equal(YourNewClass.methodName(null), null)
      // etc.
    })

    it("validates input types", () => {
      // Test type handling
      assert.throws(() => YourNewClass.methodName(123))
      // OR test graceful coercion
      assert.equal(YourNewClass.methodName(123), "123")
    })

    it("throws appropriate errors", () => {
      // Test error conditions
      await assert.rejects(
        () => YourNewClass.asyncMethod("bad input"),
        /expected error message/
      )
    })
  })

  describe("error scenarios", () => {
    // Test failure modes, async rejections, etc.
  })

  describe("performance and edge cases", () => {
    // Test boundary conditions, large inputs, etc.
  })
})
```

#### **Testing Best Practices:**

1. **üéØ Test the Implementation, Not Just the Interface**
   - Don't just test that it works, test that it **handles edge cases**
   - Look for the "what if" scenarios that will break in production

2. **üö® Edge Cases Are Your Friend**

   ```javascript
   // Test ALL of these:
   YourMethod(null)
   YourMethod(undefined)
   YourMethod("")
   YourMethod([])
   YourMethod({})
   YourMethod(0)
   YourMethod(-1)
   YourMethod("   ")  // whitespace
   ```

3. **üîÑ Test Error Handling**

   ```javascript
   // If it can throw, test the throw
   await assert.rejects(() => method("bad"), ExpectedErrorType)

   // If it uses Sass, test the trace
   assert.equal(error.trace.length, 2)
   assert.match(error.trace[0], /expected context/)
   ```

4. **üìä Test Real-World Usage**

   ```javascript
   it("supports typical use cases", () => {
     // Test how people will actually use it
   })
   ```

5. **üß™ Use Descriptive Test Names**

   ```javascript
   // ‚ùå Bad
   it("works", () => {})

   // ‚úÖ Good
   it("returns empty string when input is null", () => {})
   ```

---

## üé® **Toolkit-Specific Testing Patterns**

### **For Classes with Sass Integration:**

```javascript
// Test that Sass errors preserve context
try {
  await YourClass.method("bad input")
  assert.fail("Should have thrown")
} catch (error) {
  assert.ok(error instanceof Sass)
  assert.equal(error.message, "original error")
  assert.match(error.trace[0], /your context/)
}
```

### **For Async Methods:**

```javascript
// Test timing if performance matters
const {result, cost} = await Util.time(async () => {
  return await YourClass.slowMethod()
})
assert.ok(cost < 100) // Or whatever makes sense
```

### **For File System Operations:**

```javascript
// Use test fixtures, not real files
const fixturePath = TestUtils.getFixturePath("test.json")
// Test with copies if you modify files
```

---

## üèÉ‚Äç‚ôÇÔ∏è **Testing Workflow**

1. **Write the failing test first** (TDD if you're into that)
2. **Implement to make it pass**
3. **Run your specific test**: `node tests/unit/YourClass.test.js`
4. **Run all tests**: Check you didn't break anything
5. **Run the linter**: `npm run lint` (fix issues)
6. **Update documentation** if needed

---

## üö® **Red Flags to Test For**

Based on issues found in this toolkit:

- **Destructuring from `null`** ‚Üí Use `[]` instead
- **Silent type coercion** ‚Üí Document or validate
- **Missing error context** ‚Üí Ensure Sass traces are meaningful
- **Regex edge cases** ‚Üí Test malformed inputs
- **Async error wrapping** ‚Üí Test double-wrapping scenarios
- **File path resolution** ‚Üí Test relative/absolute path edge cases

---

## üéØ **Quality Standards**

Your tests should:

- ‚úÖ **Cover edge cases** that real users will hit
- ‚úÖ **Validate error scenarios** thoroughly
- ‚úÖ **Match the toolkit's personality** (use the same patterns)
- ‚úÖ **Be readable** by future humans and robots
- ‚úÖ **Actually catch bugs** (not just test happy paths)

Remember: **Good tests are like good sass - they catch problems early and give you attitude when something's wrong.** üòè

---

---


*Now go forth and test with confidence! And remember - TESTING.txt is still fucking gold.* ‚ú®
